{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L62G7LTwNzoD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcl0QHvDjTV"
      },
      "source": [
        "# Get the dataset\n",
        "\n",
        "Start by getting the dataset containing Amazon and Yelp reviews, with their related sentiment (1 for positive, 0 for negative). This dataset was originally extracted from [here](https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCOtiRJZbxCH",
        "outputId": "633d1d9f-4dc7-464f-8282-963e8d0098b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P -O /tmp/sentiment.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XuqER_KMD-xS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('./sentiment.csv')\n",
        "\n",
        "# Extract out sentences and labels\n",
        "sentences = dataset['text'].tolist()\n",
        "labels = dataset['sentiment'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbsx1T2CXPNO",
        "outputId": "96f30ba3-66df-4df3-c212-fe59f87ac549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So there is no way for me to plug it in here in the US unless I go by a converter.\n",
            "0\n",
            "\n",
            "\n",
            "Good case Excellent value.\n",
            "1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print some example sentences and labels\n",
        "for x in range(2):\n",
        "  print(sentences[x])\n",
        "  print(labels[x])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33AthPiALFZK"
      },
      "source": [
        "#Create a subwords dataset\n",
        "\n",
        "We will use the Amazon and Yelp reviews dataset with tensorflow_datasets's SubwordTextEncoder functionality. \n",
        "\n",
        "SubwordTextEncoder.build_from_corpus() will create a tokenizer for us. You could also use this functionality to get subwords from a much larger corpus of text as well, but we'll just use our existing dataset here.\n",
        "\n",
        "We'll create a subword vocab_size of only the 1,000 most common subwords, as well as cutting off each subword to be at most 5 characters.\n",
        "\n",
        "Check out the related documentation for the the subword text encoder [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder#build_from_corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NaicNCcLYyf",
        "outputId": "7a41faa6-f3a0-402f-b49e-255fd86866ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size is  999\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "vocab_size = 1000\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(sentences, vocab_size, max_subword_length=5)\n",
        "\n",
        "# How big is the vocab size?\n",
        "print(\"Vocab size is \", tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvRVoeIVLevh",
        "outputId": "1333327f-27ea-46f0-a7d6-f584896e8cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have to jiggle the plug to get it to line up right to get decent volume.\n",
            "[4, 31, 6, 849, 162, 450, 12, 1, 600, 438, 775, 6, 175, 14, 6, 55, 213, 159, 474, 775, 6, 175, 614, 380, 295, 148, 72, 789]\n"
          ]
        }
      ],
      "source": [
        "# Check that the tokenizer works appropriately\n",
        "num = 5\n",
        "print(sentences[num])\n",
        "encoded = tokenizer.encode(sentences[num])\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_vacTCifklV",
        "outputId": "cb254156-9b20-4e0e-9fe7-d7ebf69a7561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I \n",
            "have \n",
            "to \n",
            "j\n",
            "ig\n",
            "gl\n",
            "e \n",
            "the \n",
            "pl\n",
            "ug\n",
            " \n",
            "to \n",
            "get \n",
            "it \n",
            "to \n",
            "li\n",
            "ne \n",
            "up \n",
            "right\n",
            " \n",
            "to \n",
            "get \n",
            "dec\n",
            "ent \n",
            "vo\n",
            "lu\n",
            "me\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# Separately print out each subword, decoded\n",
        "for i in encoded:\n",
        "  print(tokenizer.decode([i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT528cptLupl"
      },
      "source": [
        "## Replace sentence data with encoded subwords\n",
        "\n",
        "Now, we'll create the sequences to be used for training by actually encoding each of the individual sentences. This is equivalent to `text_to_sequences` with the `Tokenizer` we used in earlier exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lkseMhxjL09F"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "  sentences[i] = tokenizer.encode(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y21yRuzmL43U",
        "outputId": "36f4ac1e-0198-4214-c55f-8f2a13c323e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 31, 6, 849, 162, 450, 12, 1, 600, 438, 775, 6, 175, 14, 6, 55, 213, 159, 474, 775, 6, 175, 614, 380, 295, 148, 72, 789]\n"
          ]
        }
      ],
      "source": [
        "# Check the sentences are appropriately replaced\n",
        "print(sentences[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HrcPHESMBMs"
      },
      "source": [
        "## Final pre-processing\n",
        "\n",
        "Before training, we still need to pad the sequences, as well as split into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "50-hTsogLSL-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "max_length = 50\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "# Pad all sequences\n",
        "sequences_padded = pad_sequences(sentences, maxlen=max_length, \n",
        "                                 padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Separate out the sentences and labels into training and test sets\n",
        "training_size = int(len(sentences) * 0.8)\n",
        "\n",
        "training_sequences = sequences_padded[0:training_size]\n",
        "testing_sequences = sequences_padded[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]\n",
        "\n",
        "# Make labels into numpy arrays for use with the network later\n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qg-maex27KPW"
      },
      "outputs": [],
      "source": [
        "# Use the model to predict some reviews   \n",
        "fake_reviews = [\"I love this phone\", \n",
        "                \"Everything was cold\",\n",
        "                \"Everything was hot exactly as I wanted\", \n",
        "                \"Everything was green\", \n",
        "                \"the host seated us immediately\",\n",
        "                \"they gave us free chocolate cake\", \n",
        "                \"we couldn't hear each other talk because of the shouting in the kitchen\"\n",
        "              ]\n",
        "\n",
        "# predict_review(model, fake_reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhLPbUl2AZ0y"
      },
      "source": [
        "# Plot the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jzBM1PpJAYfD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwr5inBiWffb"
      },
      "source": [
        "# Define a function to predict the sentiment of reviews\n",
        "\n",
        "We'll be creating models with some differences and will use each model to predict the sentiment of some new reviews.\n",
        "\n",
        "To save time, create a function that will take in a model and some new reviews, and print out the sentiment of each reviews.\n",
        "\n",
        "The higher the sentiment value is to 1, the more positive the review is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aPNOYiiaha2y"
      },
      "outputs": [],
      "source": [
        "# Define a function to take a series of reviews\n",
        "# and predict whether each one is a positive or negative review\n",
        "\n",
        "# max_length = 100 # previously defined\n",
        "\n",
        "def predict_review(new_sentences, maxlen=100, show_padded_sequence=True ):\n",
        "  # Keep the original sentences so that we can keep using them later\n",
        "  # Create an array to hold the encoded sequences\n",
        "  model = tf.load(\"./BLSTM_model1.h5\")\n",
        "  new_sequences = []\n",
        "\n",
        "  # Convert the new reviews to sequences\n",
        "  for i, frvw in enumerate(new_sentences):\n",
        "    new_sequences.append(tokenizer.encode(frvw))\n",
        "\n",
        "  trunc_type='post' \n",
        "  padding_type='post'\n",
        "\n",
        "  # Pad all sequences for the new reviews\n",
        "  new_reviews_padded = pad_sequences(new_sequences, maxlen=max_length, \n",
        "                                 padding=padding_type, truncating=trunc_type)             \n",
        "\n",
        "  classes = model.predict(new_reviews_padded)\n",
        "\n",
        "  # The closer the class is to 1, the more positive the review is\n",
        "  for x in range(len(new_sentences)):\n",
        "    \n",
        "    # We can see the padded sequence if desired\n",
        "    # Print the sequence\n",
        "    if (show_padded_sequence):\n",
        "      print(new_reviews_padded[x])\n",
        "    # Print the review as text\n",
        "    print(new_sentences[x])\n",
        "    # Print its predicted class\n",
        "    print(classes[x])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJKbMq3K4iy"
      },
      "source": [
        "# Define a function to train and show the results of models with different layers\n",
        "\n",
        "In the rest of this colab, we will define models, and then see the results. \n",
        "\n",
        "Define a function that will take the model, compile it, train it, graph the accuracy and loss, and then predict some results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PevUcINXK3gn"
      },
      "outputs": [],
      "source": [
        "def fit_model_now (model, sentences) :\n",
        "  num_epochs = 30\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  history = model.fit(training_sequences, training_labels_final, epochs=num_epochs, \n",
        "                      validation_data=(testing_sequences, testing_labels_final))\n",
        "  return history\n",
        "\n",
        "def plot_results (history):\n",
        "  plot_graphs(history, \"accuracy\")\n",
        "  plot_graphs(history, \"loss\")\n",
        "\n",
        "def fit_model_and_show_results (model, sentences):\n",
        "  history = fit_model_now(model, sentences)\n",
        "  plot_results(history)\n",
        "  predict_review(model, sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsxKPbCnPJTj"
      },
      "source": [
        "# Use multiple bidirectional layers\n",
        "\n",
        "Now let's see if we get any improvements from adding another Bidirectional LSTM layer to the model.\n",
        "\n",
        "Notice that the first Bidirectionl LSTM layer returns a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N6Zul47PMED"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model_multiple_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n",
        "                                                       return_sequences=True)), \n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "fit_model_and_show_results(model_multiple_bidi_lstm, fake_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6XebrXt0jtOy"
      },
      "outputs": [],
      "source": [
        "my_reviews =[\"lovely\", \"dreadful\", \"stay away\",\n",
        "             \"everything was hot exactly as I wanted\",\n",
        "             \"everything was not exactly as I wanted\",\n",
        "             \"they gave us free chocolate cake\",\n",
        "             \"I've never eaten anything so spicy in my life, my throat burned for hours\",\n",
        "             \"for a phone that is as expensive as this one I expect it to be much easier to use than this thing is\",\n",
        "             \"we left there very full for a low price so I'd say you just can't go wrong at this place\",\n",
        "             \"that place does not have quality meals and it isn't a good place to go for dinner\",\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81v1r3Y2BwvC",
        "outputId": "c02f785e-9d8b-4507-9acf-664190fa128e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            " With two bidirectional LSTMs:\n",
            " ===================================\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "lovely\n",
            "[0.99367046]\n",
            "\n",
            "\n",
            "dreadful\n",
            "[0.00417532]\n",
            "\n",
            "\n",
            "stay away\n",
            "[0.00437907]\n",
            "\n",
            "\n",
            "everything was hot exactly as I wanted\n",
            "[0.99061173]\n",
            "\n",
            "\n",
            "everything was not exactly as I wanted\n",
            "[0.00579483]\n",
            "\n",
            "\n",
            "they gave us free chocolate cake\n",
            "[0.9873853]\n",
            "\n",
            "\n",
            "I've never eaten anything so spicy in my life, my throat burned for hours\n",
            "[0.04252017]\n",
            "\n",
            "\n",
            "for a phone that is as expensive as this one I expect it to be much easier to use than this thing is\n",
            "[0.00378881]\n",
            "\n",
            "\n",
            "we left there very full for a low price so I'd say you just can't go wrong at this place\n",
            "[0.0785692]\n",
            "\n",
            "\n",
            "that place does not have quality meals and it isn't a good place to go for dinner\n",
            "[0.00550365]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"===================================\\n\",\"With two bidirectional LSTMs:\\n\", \"===================================\")\n",
        "predict_review(model_multiple_bidi_lstm, my_reviews, show_padded_sequence=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "537c77e7e1dd99e6b90ae1f2a20016ebdcd0ffa43ec37fcbf67afedf08f03998"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
